{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "83b869a0-b028-432d-be51-8500727d0dc0",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torchvision\n",
    "import os\n",
    "import cv2\n",
    "import numpy as np\n",
    "from torch import nn\n",
    "from glob import glob\n",
    "import timm\n",
    "import torch.nn.functional as F\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ae1c227e-9322-4271-8e3e-30485075630c",
   "metadata": {},
   "outputs": [],
   "source": [
    "device='cpu'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e54c4217-35c4-4703-ab13-b1502e93f480",
   "metadata": {},
   "outputs": [],
   "source": [
    "image_paths = './Data/*/*'\n",
    "image_paths = glob(image_paths)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b850eaa0-28be-4007-ad49-59147f046e8e",
   "metadata": {},
   "outputs": [],
   "source": [
    "labels = []\n",
    "images = []\n",
    "\n",
    "for image_path in image_paths:\n",
    "    label = image_path.split(os.path.sep)[2]\n",
    "    image = image_path.split(os.path.sep)[3]\n",
    "    labels.append(label)\n",
    "    images.append(image)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "62a044c9-6332-458f-b0fa-6ede50dbbc4e",
   "metadata": {},
   "outputs": [],
   "source": [
    "un_labels = np.array(['You Are Not At Risk Of Blindness. No DR!', 'You Have Mild DR. Be Careful!', 'You Have Moderate DR. Consult An Opthalmologist On Future Action', 'You Have Severe DR. Seek Help Immediately!', 'You Have Profilerative DR! You Are At High Risk Of Blindness; Take Immediate Action!'], dtype='str')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "40872365-df35-4601-a433-3da3e0b27632",
   "metadata": {},
   "outputs": [],
   "source": [
    "label2pred = dict(zip(un_labels, range(0, 5)))\n",
    "pred2label = dict(zip(range(0, 5), un_labels))\n",
    "n_classes = len(np.unique(labels))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0cd834f1-1d8e-4f47-8ca6-6b375f709941",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ClassificationBase(nn.Module):\n",
    "    def training_step(self, batch):\n",
    "        images, labels = batch\n",
    "        images = images.to(device)\n",
    "        labels = labels.to(device)\n",
    "        out = self(images)\n",
    "        loss = F.cross_entropy(out, labels)\n",
    "        acc = accuracy(out, labels)          \n",
    "        return loss, acc\n",
    "\n",
    "    def validation_step(self, batch):\n",
    "        images, labels = batch \n",
    "        images = images.to(device)\n",
    "        labels = labels.to(device)\n",
    "        out = self(images)                    \n",
    "        loss = F.cross_entropy(out, labels) \n",
    "        acc = accuracy(out, labels)          \n",
    "        return {'val_loss': loss.detach(), 'val_acc': acc}\n",
    "        \n",
    "    def validation_epoch_end(self, outputs):\n",
    "        batch_losses = [x['val_loss'] for x in outputs]\n",
    "        epoch_loss = torch.stack(batch_losses).mean()   \n",
    "        batch_accs = [x['val_acc'] for x in outputs]\n",
    "        epoch_acc = torch.stack(batch_accs).mean()    \n",
    "        return {'val_loss': epoch_loss.item(), 'val_acc': epoch_acc.item()}\n",
    "    \n",
    "    def epoch_end(self, epoch, result):\n",
    "        print(\"Epoch [{}], val_loss: {:.4f}, val_acc: {:.4f}\\n\".format(epoch, result['val_loss'], result['val_acc']))\n",
    "        \n",
    "\n",
    "def accuracy(outputs, labels):\n",
    "    _, preds = torch.max(outputs, dim=1)\n",
    "    return torch.tensor(torch.sum(preds == labels).item() / len(preds))\n",
    "\n",
    "def evaluate(model, val_loader):\n",
    "    outputs = [model.validation_step(batch) for batch in val_loader]\n",
    "    return model.validation_epoch_end(outputs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a985bd1e-1650-4567-986a-e3dbc9eb185f",
   "metadata": {},
   "outputs": [],
   "source": [
    "from efficientnet_pytorch import EfficientNet\n",
    "\n",
    "class EfficientNetB4(ClassificationBase):\n",
    "    \n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        \n",
    "        self.network = EfficientNet.from_pretrained('efficientnet-b4')\n",
    "        self.network._fc = nn.Linear(1792, n_classes)\n",
    "        \n",
    "    def forward(self, batch):\n",
    "        batch = batch.to(device)\n",
    "        return self.network(batch)\n",
    "        \n",
    "        \n",
    "        \n",
    "model = EfficientNetB4()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "013d11e9-75a6-4038-a8b8-9df4943aee44",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = model.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8da18083-852e-4e27-961d-3fabbfacca3b",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.load_state_dict(torch.load('blindness.pth', map_location=torch.device('cpu')))\n",
    "model.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "506897bb-e09c-400a-a374-e922b82b0a8e",
   "metadata": {},
   "outputs": [],
   "source": [
    "image = cv2.imread('./Data/Proliferative/10017_left.jpeg')\n",
    "image = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)\n",
    "plt.imshow(image)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e3190bef-0742-4346-98f8-a3f647aa9f9e",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_transforms = torchvision.transforms.Compose([\n",
    "    torchvision.transforms.ToTensor(),\n",
    "    torchvision.transforms.Resize((400, 400)),\n",
    "    torchvision.transforms.CenterCrop((380)),\n",
    "    torchvision.transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225])])\n",
    "\n",
    "image = test_transforms(image)\n",
    "image = torch.reshape(image, (1, 3, 380, 380))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dec8a714-1324-4883-947e-827d1b695c06",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.eval()\n",
    "pred2label[np.argmax(model(image).cpu().detach().numpy())]"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "data-science",
   "language": "python",
   "name": "data-science"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  },
  "widgets": {
   "application/vnd.jupyter.widget-state+json": {
    "state": {
     "115caf5dcd784c27ab26b274b8a42f4b": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "1.5.0",
      "model_name": "HTMLModel",
      "state": {
       "layout": "IPY_MODEL_3ec1f39903b24a86a4a68f82ab45483c",
       "style": "IPY_MODEL_e5f61592d1874af3821ff7c8a3615075",
       "value": " 74.4M/74.4M [00:08&lt;00:00, 8.52MB/s]"
      }
     },
     "1269047227ee47a8a8aea4d56a9403c2": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "1.5.0",
      "model_name": "ProgressStyleModel",
      "state": {
       "description_width": ""
      }
     },
     "19b0977b6ca047a4a8ba015c04015ba5": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "1.2.0",
      "model_name": "LayoutModel",
      "state": {}
     },
     "2680d5a33e184ae0a7337db210193496": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "1.5.0",
      "model_name": "HBoxModel",
      "state": {
       "children": [
        "IPY_MODEL_ec83e605851c4164a20b5f5736a61b85",
        "IPY_MODEL_aae7f75eaa6f46c6a58548dc6b714c87",
        "IPY_MODEL_115caf5dcd784c27ab26b274b8a42f4b"
       ],
       "layout": "IPY_MODEL_89aefb428524422bb311fd7251d33cb9"
      }
     },
     "306621051151492886407aad2ab2fd36": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "1.5.0",
      "model_name": "DescriptionStyleModel",
      "state": {
       "description_width": ""
      }
     },
     "3ec1f39903b24a86a4a68f82ab45483c": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "1.2.0",
      "model_name": "LayoutModel",
      "state": {}
     },
     "6d8bf9297d094fe2bc61b4046fe203a9": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "1.2.0",
      "model_name": "LayoutModel",
      "state": {}
     },
     "89aefb428524422bb311fd7251d33cb9": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "1.2.0",
      "model_name": "LayoutModel",
      "state": {}
     },
     "aae7f75eaa6f46c6a58548dc6b714c87": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "1.5.0",
      "model_name": "FloatProgressModel",
      "state": {
       "bar_style": "success",
       "layout": "IPY_MODEL_19b0977b6ca047a4a8ba015c04015ba5",
       "max": 77999237,
       "style": "IPY_MODEL_1269047227ee47a8a8aea4d56a9403c2",
       "value": 77999237
      }
     },
     "e5f61592d1874af3821ff7c8a3615075": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "1.5.0",
      "model_name": "DescriptionStyleModel",
      "state": {
       "description_width": ""
      }
     },
     "ec83e605851c4164a20b5f5736a61b85": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "1.5.0",
      "model_name": "HTMLModel",
      "state": {
       "layout": "IPY_MODEL_6d8bf9297d094fe2bc61b4046fe203a9",
       "style": "IPY_MODEL_306621051151492886407aad2ab2fd36",
       "value": "100%"
      }
     }
    },
    "version_major": 2,
    "version_minor": 0
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
