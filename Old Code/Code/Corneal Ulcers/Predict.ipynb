{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "6554c2c0-3c53-4a7b-a18e-6f69cbb91cf1",
   "metadata": {},
   "source": [
    "# Making Predictions On A Single Image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "00853a49-9794-46cb-9fa4-a8cf1b54424f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torchvision\n",
    "import os\n",
    "import cv2\n",
    "import numpy as np\n",
    "from torch import nn\n",
    "from glob import glob\n",
    "import timm\n",
    "import torch.nn.functional as F\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3a6299a8-21df-42a5-8b14-b548a5df57e5",
   "metadata": {},
   "outputs": [],
   "source": [
    "device='cpu'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cb4abb1a-b2c6-4e5f-b3d3-2d68063dd702",
   "metadata": {},
   "outputs": [],
   "source": [
    "image_paths = './Data/images/*/*'\n",
    "image_paths = glob(image_paths)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c948309e-ddb2-42ad-8e6b-5b8d27eab8b9",
   "metadata": {},
   "outputs": [],
   "source": [
    "labels = []\n",
    "images = []\n",
    "\n",
    "for image_path in image_paths:\n",
    "    label = image_path.split(os.path.sep)[3]\n",
    "    image = image_path.split(os.path.sep)[4]\n",
    "    labels.append(label)\n",
    "    images.append(image)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7a9a7229-fb52-4425-b035-7d4abe406b79",
   "metadata": {},
   "outputs": [],
   "source": [
    "np.unique(labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2b5980cb-9ca0-4a0d-9daf-48136cfc5b17",
   "metadata": {},
   "outputs": [],
   "source": [
    "labels = np.array(labels, dtype='str')\n",
    "label2pred = dict(zip(np.unique(labels), range(0, 3)))\n",
    "pred2label = dict(zip(range(0, 3), np.unique(labels)))\n",
    "n_classes = len(np.unique(labels))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a24b1aa7-798c-463a-b2ae-7779244217d8",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ClassificationBase(nn.Module):\n",
    "    def training_step(self, batch):\n",
    "        images, labels = batch\n",
    "        images = images.to(device)\n",
    "        labels = labels.to(device)\n",
    "        out = self(images)\n",
    "        loss = F.cross_entropy(out, labels)\n",
    "        acc = accuracy(out, labels)          \n",
    "        return loss, acc\n",
    "\n",
    "    def validation_step(self, batch):\n",
    "        images, labels = batch \n",
    "        images = images.to(device)\n",
    "        labels = labels.to(device)\n",
    "        out = self(images)                    \n",
    "        loss = F.cross_entropy(out, labels) \n",
    "        acc = accuracy(out, labels)          \n",
    "        return {'val_loss': loss.detach(), 'val_acc': acc}\n",
    "        \n",
    "    def validation_epoch_end(self, outputs):\n",
    "        batch_losses = [x['val_loss'] for x in outputs]\n",
    "        epoch_loss = torch.stack(batch_losses).mean()   \n",
    "        batch_accs = [x['val_acc'] for x in outputs]\n",
    "        epoch_acc = torch.stack(batch_accs).mean()    \n",
    "        return {'val_loss': epoch_loss.item(), 'val_acc': epoch_acc.item()}\n",
    "    \n",
    "    def epoch_end(self, epoch, result):\n",
    "        print(\"Epoch [{}], val_loss: {:.4f}, val_acc: {:.4f}\\n\".format(epoch, result['val_loss'], result['val_acc']))\n",
    "        \n",
    "\n",
    "def accuracy(outputs, labels):\n",
    "    _, preds = torch.max(outputs, dim=1)\n",
    "    return torch.tensor(torch.sum(preds == labels).item() / len(preds))\n",
    "\n",
    "def evaluate(model, val_loader):\n",
    "    outputs = [model.validation_step(batch) for batch in val_loader]\n",
    "    return model.validation_epoch_end(outputs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9cc0a77a-1bd8-452a-b66b-1b8935158b8a",
   "metadata": {},
   "outputs": [],
   "source": [
    "class EfficientNetB4(ClassificationBase):\n",
    "    \n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        \n",
    "        self.network = torchvision.models.efficientnet_b4(weights=True)\n",
    "        num_ftrs = self.network.classifier\n",
    "        self.network.classifier = nn.Sequential(nn.Dropout(0.3), nn.Linear(1792, n_classes))\n",
    "        \n",
    "    def forward(self, batch):\n",
    "        batch = batch.to(device)\n",
    "        return self.network(batch)\n",
    "        \n",
    "        \n",
    "        \n",
    "model = EfficientNetB4()     "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c02862a9-3a93-4cd8-8cc6-7c117e4ef62d",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = model.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "136b5b9e-783c-44b9-b0d8-29f926f099b4",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.load_state_dict(torch.load('EfficientNet-0.97.pth', map_location=torch.device('cpu')))\n",
    "model.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9a354f78-382f-41e8-ad6a-db2d8fa3b9f5",
   "metadata": {},
   "outputs": [],
   "source": [
    "image = cv2.imread('./Data/images/point-flaky/359.jpg')\n",
    "image = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)\n",
    "plt.imshow(image)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "31fac446-92d1-4c68-9120-cf1b4bf3244d",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_transforms = torchvision.transforms.Compose([\n",
    "    torchvision.transforms.ToTensor(),\n",
    "    torchvision.transforms.Resize((400, 400)),\n",
    "    torchvision.transforms.CenterCrop((380))\n",
    "])\n",
    "\n",
    "image = test_transforms(image)\n",
    "image = torch.reshape(image, (1, 3, 380, 380))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "457af0f3-ee86-4cd3-bb6e-5aaa88ab2c0b",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.eval()\n",
    "pred2label[np.argmax(model(image).cpu().detach().numpy())]"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "data-science",
   "language": "python",
   "name": "data-science"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  },
  "widgets": {
   "application/vnd.jupyter.widget-state+json": {
    "state": {},
    "version_major": 2,
    "version_minor": 0
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
